{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22d68f1",
   "metadata": {},
   "source": [
    "## Set up environments\n",
    "\n",
    "(Optional)\n",
    "'''sudo chown -R jinhol:jinhol models'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f18a3",
   "metadata": {},
   "source": [
    "- Link: https://github.com/triton-inference-server/backend/blob/main/README.md#backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2918e4d",
   "metadata": {},
   "source": [
    "```docker run --gpus='\"device=1\"' --rm -p8000:8000 -p8001:8001 -p8002:8002 -v $(pwd)/models:/models nvcr.io/nvidia/tritonserver:22.03-py3 tritonserver --model-repository=/models\n",
    "                    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd40ea2",
   "metadata": {},
   "source": [
    "## Model Repository\n",
    "Model repository is the most basic and important concept on Triton.\n",
    "\n",
    "https://github.com/triton-inference-server/server/tree/main/docs/examples/model_repository\n",
    "\n",
    "You can also add ```--strict-model-config=false```\n",
    "\n",
    "Please see this link for more detail info.\n",
    "\n",
    "https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#auto-generated-model-configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e27af",
   "metadata": {},
   "source": [
    "## Check Model's setting&configuration\n",
    "\n",
    "```curl localhost:8000/v2/models/<model name>```\n",
    "\n",
    "```curl localhost:8000/v2/models/<model name>/config```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0ceb8",
   "metadata": {},
   "source": [
    "## Triton's Client\n",
    "\n",
    "You can find more useful examples at https://github.com/triton-inference-server/client/tree/main/src/python/examples\n",
    "\n",
    "```docker run -it -v $(pwd):/hands_on --gpus '\"device=2\"' --net=host nvcr.io/nvidia/tritonserver:22.03-py3-sdk```\n",
    "\n",
    "```cd /hands_on```\n",
    "\n",
    "```pip install torchvision```\n",
    "\n",
    "```python client.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b323c0",
   "metadata": {},
   "source": [
    "## Model Loading: None, Poll, Explicit(Recommended)\n",
    "\n",
    "https://github.com/triton-inference-server/server/blob/main/docs/model_management.md#model-control-mode-explicit\n",
    "\n",
    "https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md\n",
    "    \n",
    "```curl -X POST localhost:8000/v2/repository/index```\n",
    "\n",
    "```curl -X POST localhost:8000/v2/repository/models/trt_model/load```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55af60",
   "metadata": {},
   "source": [
    "## Perf Analyzer\n",
    "\n",
    "https://github.com/triton-inference-server/server/blob/main/docs/perf_analyzer.md\n",
    "\n",
    "[Example]\n",
    "\n",
    "```perf_analyzer -u localhost:8000 -m trt_model -b 1 --percentile=95  --concurrency-range 1:4 -f perf.csv```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459bf2b",
   "metadata": {},
   "source": [
    "## Config.pbtxt Optimization\n",
    "\n",
    "\n",
    "https://github.com/triton-inference-server/server/blob/main/docs/optimization.md\n",
    "\n",
    "\n",
    "1. Instance Group\n",
    "\n",
    "```\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "2. Dynamic Batching\n",
    "\n",
    "```\n",
    "dynamic_batching { \n",
    "  preferred_batch_size: [ 4, 8, 16, 32 ] \n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a2a2b",
   "metadata": {},
   "source": [
    "## Model Analyzer\n",
    "\n",
    "Follow [installation guide](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/install.md)\n",
    "\n",
    "\n",
    "```docker run -it --rm --gpus '\"device=1\"' \\\n",
    "    -v $(pwd)/models:/models \\\n",
    "    --net=host --name model-analyzer \\\n",
    "    model-analyzer /bin/bash```\n",
    "\n",
    "\n",
    "```model-analyzer profile --model-repository /models --profile-models trt_model --run-config-search-max-concurrency 2 --run-config-search-max-instance-count 2 --override-output-model-repository```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a52f6",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "http://localhost:8002/metrics\n",
    "        \n",
    "Those metrics are compatible with [Prometheus](https://prometheus.io/)\n",
    "\n",
    "Please see our reference example likes:\n",
    "\n",
    "https://github.com/triton-inference-server/server/tree/main/deploy/k8s-onprem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41ab10",
   "metadata": {},
   "source": [
    "## FasterTransformer\n",
    "\n",
    "https://github.com/NVIDIA/FasterTransformer\n",
    "\n",
    "https://github.com/triton-inference-server/fastertransformer_backend/tree/main/all_models/gpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
